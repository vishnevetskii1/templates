{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e376402",
   "metadata": {},
   "source": [
    "# Обработка текстовых данных\n",
    "\n",
    "## Вступление\n",
    "Многие практические задачи так или иначе могут вовлекать в себя работу с текстовыми данными, например:\n",
    "\n",
    "- классификация текстов\n",
    "    - анализ тональности (например, позитивный/негативный отзыв)\n",
    "    - фильтрация спама\n",
    "    - по теме или жанру\n",
    "- машинный перевод\n",
    "- распознавание и синтез речи\n",
    "- извлечение информации\n",
    "    - именованные сущности (например, извлечение имен, локаций, названий организаций)\n",
    "    - извлечение фактов и событий\n",
    "- кластеризация текстов\n",
    "- оптическое распознавание символов\n",
    "- проверка правописания\n",
    "- вопросно-ответные системы, информационный поиск\n",
    "- суммаризация текстов\n",
    "- генерация текстов\n",
    "\n",
    "В целом, алгоритм работы с текстовыми данными можно разбить на такие шаги:\n",
    "\n",
    "- предобработка сырых данных\n",
    "- токенизация (создание словаря)\n",
    "- обработка словаря (удаление стоп-слов и пунктуации)\n",
    "- обработка токенов (лемматизация / стемминг)\n",
    "- векторизация текста (bag of words, TF-IDF, etc)\n",
    "\n",
    "### Структура ноутбука:\n",
    "1. Токенизация\n",
    "2. Стоп-слова и пунктуация\n",
    "3. Лемматизация и стемминг\n",
    "4. Bag-of-words и TD-IDF\n",
    "5. Решение задачи с текстовыми данными\n",
    "6. Регулярные выражения\n",
    "7. Немножко про категориальные признаки\n",
    "8. Полный пошаговый пайплайн работы с текстом\n",
    "\n",
    "**Для работы с тектом я буду в основном пользоваться библиотекой `nltk` (Natural Language Toolkit) — это популярная и очень мощная библиотека Python для обработки и анализа естественного языка (NLP).**\n",
    "\n",
    "*Что можно реализовать с помощью nltk:*\n",
    "\n",
    "- разбиение текста на предложения и слова (токенизация)\n",
    "- удаление стоп-слов\n",
    "- стемминг (приведение слова к корню)\n",
    "- лемматизация (приведение к начальной форме)\n",
    "- определение частей речи\n",
    "- обработка синтаксиса и деревьев разбора\n",
    "- работа с большими текстовыми корпусами (коллекциями текстов)\n",
    "- построение частотных словарей, анализ n-грамм\n",
    "- подготовка текста для последующего машинного обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743c721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка библиотеки nltk\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9eedb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка библиотеки nltk\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a705fa7",
   "metadata": {},
   "source": [
    "## 1. Токенизация\n",
    "\n",
    "Токенизировать — значит, поделить текст на отдельные смысловые единицы (как правило слова, но не обязательно), или по другому *токены*. Самый тривиальный способ токенизировать текст — разделить на слова по пробелам с помощью `split`. Но `split` упускает очень много всего, например, не отделяет пунктуацию от слов. Кроме этого есть ещё много менее очевидных проблем, возникающих при такой токенизации, поэтому на практике всегда используют готовые токенизаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e22e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afbbe57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c48045fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Но не каждый хочет что-то исправлять:(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b75e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Но не каждый хочет что-то исправлять:(\n",
      "Токенизация c помощью split(): ['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять:(']\n",
      "Токенизация c помощью готового токенизатора: ['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':', '(']\n"
     ]
    }
   ],
   "source": [
    "print('Исходный текст:', example)\n",
    "print('Токенизация c помощью split():', example.split())\n",
    "print('Токенизация c помощью готового токенизатора:', word_tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc968a",
   "metadata": {},
   "source": [
    "В `nltk` есть много разных токенизаторов, посмотреть на них можно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c54686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего встроенных в nltk токенизаторов: 61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['BlanklineTokenizer',\n",
       " 'LegalitySyllableTokenizer',\n",
       " 'LineTokenizer',\n",
       " 'MWETokenizer',\n",
       " 'NLTKWordTokenizer',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'PunktTokenizer',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'SExprTokenizer']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "print('Всего встроенных в nltk токенизаторов:', len(dir(tokenize)))\n",
    "#первые 10\n",
    "dir(tokenize)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04250c8c",
   "metadata": {},
   "source": [
    "Можно получить индексы начала и конца каждого токена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f6aa62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (3, 5), (6, 12), (13, 18), (19, 25), (26, 38)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_tok = tokenize.WhitespaceTokenizer()\n",
    "list(wh_tok.span_tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede01dc",
   "metadata": {},
   "source": [
    "Некторые токенизаторы ведут себя специфично:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f14809b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', \"n't\", 'stop', 'me']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f7da9",
   "metadata": {},
   "source": [
    "Для некоторых задач это может быть полезно.\n",
    "\n",
    "А некоторые предназначены вообще не для текста на естественном языке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd7d80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(a (b c))', 'd', 'e', '(f)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.SExprTokenizer().tokenize(\"(a (b c)) d e (f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358f2ba",
   "metadata": {},
   "source": [
    "Есть токенизатор, который может быть полезен для работы с твитами или сообщениями из соц. сетей. Он сохранит смайлики, хештеги и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f136e47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':(']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tw = TweetTokenizer()\n",
    "tw.tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e8622",
   "metadata": {},
   "source": [
    "## 3. Стоп-слова и пунктуация\n",
    "\n",
    "*Стоп-слова* — это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "976f8824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "020dbc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f84ebde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "191fc192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#список русских стоп слов\n",
    "rus_stop_words_list = ['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его',\n",
    "                        'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', \n",
    "                        'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже',\n",
    "                        'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом',\n",
    "                        'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их',\n",
    "                        'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда',  \n",
    "                        'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', \n",
    "                        'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об',\n",
    "                        'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', \n",
    "                        'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя',\n",
    "                        'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n",
    "#список английских стоп слов\n",
    "eng_stop_words_list = ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as',\n",
    "                        'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd',\n",
    "                        'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', \n",
    "                        'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\",\n",
    "                        'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', \n",
    "                        'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more',\n",
    "                        'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only',\n",
    "                        'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \n",
    "                        \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', \n",
    "                        'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', \n",
    "                        'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', \n",
    "                        'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", \n",
    "                        'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16c6d97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "#знаки пунктуации\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c57c49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#стоп слова + знаки пунктуации\n",
    "noise = stopwords.words(\"russian\") + list(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db15d0",
   "metadata": {},
   "source": [
    "## 3. Лемматизация и стемминг\n",
    "### 3.1 Лемматизация\n",
    "\n",
    "[**Лемматизация**](https://en.wikipedia.org/wiki/Lemmatisation) — процесс приведения слова к его нормальной форме (**лемме**):\n",
    "- для существительных — именительный падеж, единственное число;\n",
    "- для прилагательных — именительный падеж, единственное число, мужской род;\n",
    "- для глаголов, причастий, деепричастий — глагол в инфинитиве.\n",
    "\n",
    "Например, токены «пью», «пил», «пьет» перейдут в «пить». Почему это хорошо?\n",
    "* Во-первых, мы хотим рассматривать как отдельный признак каждое *слово*, а не каждую его отдельную форму.\n",
    "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лемматизации выкидываем мы только её.\n",
    "\n",
    "Для русского есть два хороших лемматизатора: `mystem` и `pymorphy`.\n",
    "\n",
    "#### [Mystem](https://tech.yandex.ru/mystem/)\n",
    "\n",
    "Mystem — это пример популярной библиотеки для лемматизации. Как с ним работать:\n",
    "* скачать mystem и запускать [из терминала с разными параметрами](https://tech.yandex.ru/mystem/doc/)\n",
    "* использовать обёртку для питона [pymystem3](https://pythonhosted.org/pymystem3/pymystem3.html) (она медленнее, но удобнее в использовании)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5041530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee301dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem_analyzer = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cb2034",
   "metadata": {},
   "source": [
    "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
    "* mystem_bin — путь к `mystem`, если их несколько\n",
    "* grammar_info — нужна ли грамматическая информация или только леммы (по умолчанию нужна)\n",
    "* disambiguation — нужно ли снятие [омонимии](https://ru.wikipedia.org/wiki/%D0%9E%D0%BC%D0%BE%D0%BD%D0%B8%D0%BC%D1%8B) - дизамбигуация (по умолчанию нужна)\n",
    "* entire_input — нужно ли сохранять в выводе все (пробелы, например), или можно выкинуть (по умолчанию оставляется все)\n",
    "\n",
    "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
    "\n",
    "Можно просто лемматизировать текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b39b80ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['но', ' ', 'не', ' ', 'каждый', ' ', 'хотеть', ' ', 'что-то', ' ', 'исправлять', ':(\\n']\n"
     ]
    }
   ],
   "source": [
    "print(mystem_analyzer.lemmatize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a0a5f",
   "metadata": {},
   "source": [
    "### [Pymorphy](http://pymorphy3.readthedocs.io/en/latest/)\n",
    "Это модуль на питоне, довольно быстрый и с кучей функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b26bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a00444cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy3 import MorphAnalyzer\n",
    "\n",
    "pymorphy3_analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e236ab",
   "metadata": {},
   "source": [
    "pymorphy3 работает с отдельными словами. Если дать ему на вход предложение, то он его просто не лемматизирует, т.к. не понимает.\n",
    "\n",
    "Метод MorphAnalyzer.parse() принимает слово и возвращает все возможные его разборы.\n",
    "\n",
    "У каждого разбора есть тег. Тег — это набор граммем, характеризующих данное слово. Например, тег 'VERB,perf,intr plur,past,indc' означает, что слово — глагол (VERB) совершенного вида (perf), непереходный (intr), множественного числа (plur), прошедшего времени (past), изъявительного наклонения (indc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "170767de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='хочет', tag=OpencorporaTag('VERB,impf,tran sing,3per,pres,indc'), normal_form='хотеть', score=1.0, methods_stack=((DictionaryAnalyzer(), 'хочет', 3136, 5),))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana = pymorphy3_analyzer.parse(\"хочет\")\n",
    "ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2786edc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хотеть'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8403bc84",
   "metadata": {},
   "source": [
    "### mystem vs. pymorphy\n",
    "\n",
    "1) *mystem работает невероятно медленно под windows на больших текстах\n",
    "\n",
    "3) *Снятие омонимии*. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy3 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8eae0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "homonym1 = \"За время обучения я прослушал больше сорока курсов.\"\n",
    "homonym2 = \"Сорока своровала блестящее украшение со стола.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "089abee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': [{'lex': 'сорок', 'wt': 0.8710292664, 'gr': 'NUM=(пр|дат|род|твор)'}], 'text': 'сорока'}\n",
      "{'analysis': [{'lex': 'сорока', 'wt': 0.1210970041, 'gr': 'S,жен,од=им,ед'}], 'text': 'Сорока'}\n"
     ]
    }
   ],
   "source": [
    "# корректно определил части речи\n",
    "# NUM — числительное\n",
    "# S — существительное\n",
    "print(mystem_analyzer.analyze(homonym1)[-5])\n",
    "print(mystem_analyzer.analyze(homonym2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8754efbf",
   "metadata": {},
   "source": [
    "**mystem:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4348853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'за', 'wt': 1, 'gr': 'PR='}], 'text': 'За'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'время', 'wt': 1, 'gr': 'S,сред,неод=(вин,ед|им,ед)'}],\n",
       "  'text': 'время'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'обучение',\n",
       "    'wt': 1,\n",
       "    'gr': 'S,сред,неод=(вин,мн|род,ед|им,мн)'}],\n",
       "  'text': 'обучения'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'я', 'wt': 0.9999716281, 'gr': 'SPRO,ед,1-л=им'}],\n",
       "  'text': 'я'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'прослушивать',\n",
       "    'wt': 1,\n",
       "    'gr': 'V,пе=прош,ед,изъяв,муж,сов'}],\n",
       "  'text': 'прослушал'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'много', 'wt': 0.0002164204767, 'gr': 'ADV=срав'}],\n",
       "  'text': 'больше'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'сорок',\n",
       "    'wt': 0.8710292664,\n",
       "    'gr': 'NUM=(пр|дат|род|твор)'}],\n",
       "  'text': 'сорока'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'курс', 'wt': 0.6284122441, 'gr': 'S,муж,неод=род,мн'}],\n",
       "  'text': 'курсов'},\n",
       " {'text': '.'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_analyzer.analyze(homonym1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "121f3e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'сорока', 'wt': 0.1210970041, 'gr': 'S,жен,од=им,ед'}],\n",
       "  'text': 'Сорока'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'своровать',\n",
       "    'wt': 1,\n",
       "    'gr': 'V,сов,пе=прош,ед,изъяв,жен'}],\n",
       "  'text': 'своровала'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'блестящий',\n",
       "    'wt': 0.6831493248,\n",
       "    'gr': 'A=(вин,ед,полн,сред|им,ед,полн,сред|срав)'}],\n",
       "  'text': 'блестящее'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'украшение',\n",
       "    'wt': 1,\n",
       "    'gr': 'S,сред,неод=(вин,ед|им,ед)'}],\n",
       "  'text': 'украшение'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'со', 'wt': 1, 'gr': 'PR='}], 'text': 'со'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'стол', 'wt': 1, 'gr': 'S,муж,неод=род,ед'}],\n",
       "  'text': 'стола'},\n",
       " {'text': '.'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_analyzer.analyze(homonym2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390778f",
   "metadata": {},
   "source": [
    "**pymorphy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c68062f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='за', tag=OpencorporaTag('PREP'), normal_form='за', score=1.0, methods_stack=((DictionaryAnalyzer(), 'за', 24, 0),))]\n",
      "[Parse(word='время', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='время', score=0.861936, methods_stack=((DictionaryAnalyzer(), 'время', 563, 3),)), Parse(word='время', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='время', score=0.138063, methods_stack=((DictionaryAnalyzer(), 'время', 563, 0),))]\n",
      "[Parse(word='обучения', tag=OpencorporaTag('NOUN,inan,neut sing,gent'), normal_form='обучение', score=0.968253, methods_stack=((DictionaryAnalyzer(), 'обучения', 77, 2),)), Parse(word='обучения', tag=OpencorporaTag('NOUN,inan,neut plur,nomn'), normal_form='обучение', score=0.015873, methods_stack=((DictionaryAnalyzer(), 'обучения', 77, 13),)), Parse(word='обучения', tag=OpencorporaTag('NOUN,inan,neut plur,accs'), normal_form='обучение', score=0.015873, methods_stack=((DictionaryAnalyzer(), 'обучения', 77, 18),))]\n",
      "[Parse(word='я', tag=OpencorporaTag('NPRO,1per sing,nomn'), normal_form='я', score=1.0, methods_stack=((DictionaryAnalyzer(), 'я', 3246, 0),))]\n",
      "[Parse(word='прослушал', tag=OpencorporaTag('VERB,perf,tran masc,sing,past,indc'), normal_form='прослушать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'прослушал', 648, 1),))]\n",
      "[Parse(word='больше', tag=OpencorporaTag('COMP,Qual'), normal_form='большой', score=0.8, methods_stack=((DictionaryAnalyzer(), 'больше', 532, 32),)), Parse(word='больше', tag=OpencorporaTag('ADVB'), normal_form='больше', score=0.2, methods_stack=((DictionaryAnalyzer(), 'больше', 3, 0),))]\n",
      "[Parse(word='сорока', tag=OpencorporaTag('NUMR gent'), normal_form='сорок', score=0.68, methods_stack=((DictionaryAnalyzer(), 'сорока', 2920, 1),)), Parse(word='сорока', tag=OpencorporaTag('NOUN,anim,femn sing,nomn'), normal_form='сорока', score=0.08, methods_stack=((DictionaryAnalyzer(), 'сорока', 421, 0),)), Parse(word='сорока', tag=OpencorporaTag('NUMR ablt'), normal_form='сорок', score=0.08, methods_stack=((DictionaryAnalyzer(), 'сорока', 2920, 4),)), Parse(word='сорока', tag=OpencorporaTag('NUMR loct'), normal_form='сорок', score=0.08, methods_stack=((DictionaryAnalyzer(), 'сорока', 2920, 5),)), Parse(word='сорока', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='сорока', score=0.04, methods_stack=((DictionaryAnalyzer(), 'сорока', 44, 0),)), Parse(word='сорока', tag=OpencorporaTag('NUMR datv'), normal_form='сорок', score=0.04, methods_stack=((DictionaryAnalyzer(), 'сорока', 2920, 2),))]\n",
      "[Parse(word='курсов.', tag=OpencorporaTag('UNKN'), normal_form='курсов.', score=1.0, methods_stack=((UnknAnalyzer(), 'курсов.'),))]\n"
     ]
    }
   ],
   "source": [
    "for word in homonym1.split():\n",
    "    print(pymorphy3_analyzer.parse(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c584f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='сорока', tag=OpencorporaTag('NUMR gent'), normal_form='сорок', score=0.68, methods_stack=((DictionaryAnalyzer(), 'сорока', 2920, 1),)), Parse(word='сорока', tag=OpencorporaTag('NOUN,anim,femn sing,nomn'), normal_form='сорока', score=0.08, methods_stack=((DictionaryAnalyzer(), 'сорока', 421, 0),)), Parse(word='сорока', tag=OpencorporaTag('NUMR ablt'), normal_form='сорок', score=0.08, methods_stack=((DictionaryAnalyzer(), 'сорока', 2920, 4),)), Parse(word='сорока', tag=OpencorporaTag('NUMR loct'), normal_form='сорок', score=0.08, methods_stack=((DictionaryAnalyzer(), 'сорока', 2920, 5),)), Parse(word='сорока', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='сорока', score=0.04, methods_stack=((DictionaryAnalyzer(), 'сорока', 44, 0),)), Parse(word='сорока', tag=OpencorporaTag('NUMR datv'), normal_form='сорок', score=0.04, methods_stack=((DictionaryAnalyzer(), 'сорока', 2920, 2),))]\n",
      "[Parse(word='своровала', tag=OpencorporaTag('VERB,perf,tran femn,sing,past,indc'), normal_form='своровать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'своровала', 748, 2),))]\n",
      "[Parse(word='блестящее', tag=OpencorporaTag('ADJF,Qual neut,sing,nomn'), normal_form='блестящий', score=0.2, methods_stack=((DictionaryAnalyzer(), 'блестящее', 388, 14),)), Parse(word='блестящее', tag=OpencorporaTag('ADJF,Qual neut,sing,accs'), normal_form='блестящий', score=0.2, methods_stack=((DictionaryAnalyzer(), 'блестящее', 388, 17),)), Parse(word='блестящее', tag=OpencorporaTag('COMP,Qual'), normal_form='блестящий', score=0.2, methods_stack=((DictionaryAnalyzer(), 'блестящее', 388, 31),)), Parse(word='блестящее', tag=OpencorporaTag('PRTF,impf,intr,pres,actv neut,sing,nomn'), normal_form='блестеть', score=0.2, methods_stack=((DictionaryAnalyzer(), 'блестящее', 490, 27),)), Parse(word='блестящее', tag=OpencorporaTag('PRTF,impf,intr,pres,actv neut,sing,accs'), normal_form='блестеть', score=0.2, methods_stack=((DictionaryAnalyzer(), 'блестящее', 490, 30),))]\n",
      "[Parse(word='украшение', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='украшение', score=0.6, methods_stack=((DictionaryAnalyzer(), 'украшение', 77, 0),)), Parse(word='украшение', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='украшение', score=0.4, methods_stack=((DictionaryAnalyzer(), 'украшение', 77, 6),))]\n",
      "[Parse(word='со', tag=OpencorporaTag('PREP Vpre'), normal_form='с', score=0.992592, methods_stack=((DictionaryAnalyzer(), 'со', 393, 1),)), Parse(word='со', tag=OpencorporaTag('NOUN,inan,femn,Fixd,Abbr sing,nomn'), normal_form='со', score=0.000617, methods_stack=((DictionaryAnalyzer(), 'со', 214, 0),)), Parse(word='со', tag=OpencorporaTag('NOUN,inan,femn,Fixd,Abbr sing,gent'), normal_form='со', score=0.000617, methods_stack=((DictionaryAnalyzer(), 'со', 214, 1),)), Parse(word='со', tag=OpencorporaTag('NOUN,inan,femn,Fixd,Abbr sing,datv'), normal_form='со', score=0.000617, methods_stack=((DictionaryAnalyzer(), 'со', 214, 2),)), Parse(word='со', tag=OpencorporaTag('NOUN,inan,femn,Fixd,Abbr sing,accs'), normal_form='со', score=0.000617, methods_stack=((DictionaryAnalyzer(), 'со', 214, 3),)), Parse(word='со', tag=OpencorporaTag('NOUN,inan,femn,Fixd,Abbr sing,ablt'), normal_form='со', score=0.000617, methods_stack=((DictionaryAnalyzer(), 'со', 214, 4),)), Parse(word='со', tag=OpencorporaTag('NOUN,inan,femn,Fixd,Abbr sing,loct'), normal_form='со', score=0.000617, methods_stack=((DictionaryAnalyzer(), 'со', 214, 5),)), Parse(word='со', tag=OpencorporaTag('NOUN,inan,femn,Fixd,Abbr plur,nomn'), normal_form='со', score=0.000617, methods_stack=((DictionaryAnalyzer(), 'со', 214, 6),)), Parse(word='со', tag=OpencorporaTag('NOUN,inan,femn,Fixd,Abbr plur,gent'), normal_form='со', score=0.000617, methods_stack=((DictionaryAnalyzer(), 'со', 214, 7),)), Parse(word='со', tag=OpencorporaTag('NOUN,inan,femn,Fixd,Abbr plur,datv'), normal_form='со', score=0.000617, methods_stack=((DictionaryAnalyzer(), 'со', 214, 8),)), Parse(word='со', tag=OpencorporaTag('NOUN,inan,femn,Fixd,Abbr plur,accs'), normal_form='со', score=0.000617, methods_stack=((DictionaryAnalyzer(), 'со', 214, 9),)), Parse(word='со', tag=OpencorporaTag('NOUN,inan,femn,Fixd,Abbr plur,ablt'), normal_form='со', score=0.000617, methods_stack=((DictionaryAnalyzer(), 'со', 214, 10),)), Parse(word='со', tag=OpencorporaTag('NOUN,inan,femn,Fixd,Abbr plur,loct'), normal_form='со', score=0.000617, methods_stack=((DictionaryAnalyzer(), 'со', 214, 11),))]\n",
      "[Parse(word='стола.', tag=OpencorporaTag('UNKN'), normal_form='стола.', score=1.0, methods_stack=((UnknAnalyzer(), 'стола.'),))]\n"
     ]
    }
   ],
   "source": [
    "for word in homonym2.split():\n",
    "    print(pymorphy3_analyzer.parse(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b2c613",
   "metadata": {},
   "source": [
    "### 3.2 Стемминг\n",
    "\n",
    "В отличие от лемматизации, при применении стемминга у всех слов отбрасываются аффиксы (окончания и суффиксы), что необязательно приводит слова к формам, существующим в рассматриваемом языке. [**Snowball**](http://snowball.tartarus.org/) — фрэймворк для написания алгоритмов стемминга. Алгоритмы стемминга отличаются для разных языков и используют знания о конкретном языке: списки окончаний для разных частей речи, разных склонений и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b307d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31a851be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = word_tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c7aaf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Но не каждый хочет что-то исправлять:(\n",
      "Стемминг: но не кажд хочет что-т исправля : (\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")\n",
    "stemmed_example = [stemmer.stem(w) for w in tokenized_example]\n",
    "print('Исходный текст:', example)\n",
    "print('Стемминг:', \" \".join(stemmed_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a92f90",
   "metadata": {},
   "source": [
    "Для английского получится что-то такое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c47b7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\n",
      "\"Whenever you feel like criticizing any one,\" he told me, \"just remember that all the people in this world haven't had the advantages that you've had.\"\n",
      "Токенезация: ['In', 'my', 'younger', 'and', 'more', 'vulnerable', 'years', 'my', 'father', 'gave', 'me', 'some', 'advice', 'that', 'I', 'been', 'turning', 'over', 'in', 'my', 'mind', 'ever', 'since', 'Whenever', 'you', 'feel', 'like', 'criticizing', 'any', 'one', 'he', 'told', 'me', 'just', 'remember', 'that', 'all', 'the', 'people', 'in', 'this', 'world', 'have', 'had', 'the', 'advantages', 'that', 'you', 'had']\n"
     ]
    }
   ],
   "source": [
    "text = 'In my younger and more vulnerable years my father gave me some advice that I\\'ve been turning over in my mind ever since.\\n\"Whenever you feel like criticizing any one,\" he told me, \"just remember that all the people in this world haven\\'t had the advantages that you\\'ve had.\"'\n",
    "print('Исходный текст:', text)\n",
    "text_tokenized = [w for w in word_tokenize(text) if w.isalpha()]\n",
    "\n",
    "print('Токенезация:', text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48827f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стемминг: in my younger and more vulner year my father gave me some advic that i been turn over in my mind ever sinc whenev you feel like critic ani one he told me just rememb that all the peopl in this world have had the advantag that you had\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "text_stemmed = [stemmer.stem(w) for w in text_tokenized]\n",
    "print('Стемминг:', \" \".join(text_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cde7cd",
   "metadata": {},
   "source": [
    "## 4. Bag-of-words и TF-IDF\n",
    "\n",
    "Но как же все-таки работать с текстами, используя стандартные методы машинного обучения? Ведь нам нужны объекты выборки, которые описываются числами, а не словами. Иначе говоря, нам нужно *векторизовать* текстовые данные.\n",
    "\n",
    "### 4.1 Bag-of-words\n",
    "\n",
    "Пусть у нас имеется коллекция текстов $D = \\{d_i\\}_{i=1}^l$ (всего $l$ текстов) и словарь всех слов, встречающихся в выборке $V = \\{v_j\\}_{j=1}^d$ (всего $d$ слов). В этом случае некоторый текст $d_i$ описывается вектором $(x_{ij})_{j=1}^d,$ где\n",
    "$$x_{ij} = \\sum_{v \\in d_i} [v = v_j].$$\n",
    "\n",
    "Таким образом, текст $d_i$ описывается вектором количества вхождений каждого слова из словаря в данный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66dd5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I like my cat.\",\n",
    "    \"My cat is the most perfect cat.\",\n",
    "    \"is this cat or is this bread?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f908338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I like my cat',\n",
       " 'My cat is the most perfect cat',\n",
       " 'is this cat or is this bread']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_tokenized = [\n",
    "    \" \".join([w for w in word_tokenize(t) if w.isalpha()]) for t in texts\n",
    "]\n",
    "texts_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68ff25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fb73ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cnt_vec = CountVectorizer()\n",
    "X = cnt_vec.fit_transform(texts_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56b0b1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['like', 'my', 'cat', 'is', 'the', 'most', 'perfect', 'this', 'or', 'bread'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vec.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1f20d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "074b2d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 2, 1, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [1, 1, 2, 0, 0, 0, 1, 0, 0, 2]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b302b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bread</th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>most</th>\n",
       "      <th>my</th>\n",
       "      <th>or</th>\n",
       "      <th>perfect</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bread  cat  is  like  most  my  or  perfect  the  this\n",
       "0      0    1   0     1     0   1   0        0    0     0\n",
       "1      0    2   1     0     1   1   0        1    1     0\n",
       "2      1    1   2     0     0   0   1        0    0     2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), columns=cnt_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b341089e",
   "metadata": {},
   "source": [
    "Как мы видим у нас по какой-то причине пропало слово `I`. Это связано с тем, что `CountVectorize` для оперделения токенов использует регулярное выражение ```token_pattern=r'(?u)\\b\\w\\w+\\b'```, где \n",
    "- `\\b` — граница слова.\n",
    "- `\\w\\w+` — слово из двух или более букв/цифр.\n",
    "\n",
    "Именно поэтому все слова длиной 1 игнорируются — они не подходят под этот шаблон.\n",
    "\n",
    "Попробуем исправить парметр `token_pattern=r'(?u)\\b\\w\\w+\\b'` на `token_pattern=r'(?u)\\b\\w+\\b'`, чтобы слова длинной из 1 символа так же векторизировались."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a108f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bread</th>\n",
       "      <th>cat</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>most</th>\n",
       "      <th>my</th>\n",
       "      <th>or</th>\n",
       "      <th>perfect</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bread  cat  i  is  like  most  my  or  perfect  the  this\n",
       "0      0    1  1   0     1     0   1   0        0    0     0\n",
       "1      0    2  0   1     0     1   1   0        1    1     0\n",
       "2      1    1  0   2     0     0   0   1        0    0     2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vec_2 = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X_2 = cnt_vec_2.fit_transform(texts_tokenized)\n",
    "\n",
    "pd.DataFrame(X_2.toarray(), columns=cnt_vec_2.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae6431",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "Обычно функцию векторизации задают самостоятельно, чтобы была возможность контролировать разбиение текста на токены, удаление стоп слов и параметры векторизации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf114a",
   "metadata": {},
   "source": [
    "### 4.2 TF-IDF\n",
    "\n",
    "Заметим, что если слово часто встречается в одном тексте, но почти не встречается в других, то оно получает для данного текста большой вес, ровно так же, как и слова, которые часто встречаются в каждом тексте. Для того чтобы разделять эти такие слова, можно использовать статистическую меру TF-IDF, характеризующую важность слова для конкретного текста. \n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Term Frequency (TF) — \"Частота слова в тексте\"\n",
    "Для каждого слова из текста $d$ рассчитаем относительную частоту встречаемости в нем (Term Frequency):\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{C(t | d)}{\\sum\\limits_{k \\in d}C(k | d)},\n",
    "$$\n",
    "где \n",
    "- $C(t | d)$ — сколько раз слово $t$ встречается в тексте $d$. \n",
    "- $\\sum\\limits_{k \\in d}C(k | d)$ — сколько **всего** слов в тексте $d$ (с учётом повторяющихся). \n",
    "\n",
    "TF показывает, $\\underline{насколько}$ $\\underline{часто}$ слово встречается в данном тексте.  \n",
    "\n",
    "**Пример:**  В тексте из 100 слов слово \"машина\" встречается 4 раза: - TF(\"машина\", d) = 4 / 100 = 0.04 (4% от всех слов)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Inverse Document Frequency (IDF) — \"Обратная частота в других текстах\"\n",
    "Также для каждого слова из текста $d$ рассчитаем обратную частоту встречаемости в корпусе текстов $D$ (Inverse Document Frequency):\n",
    "$$\n",
    "\\text{IDF}(t, D) = \\log\\left(\\frac{|D|}{|\\{d_i \\in D \\mid t \\in d_i\\}|}\\right)\n",
    "$$\n",
    "где\n",
    "- $|D|$ — общее количество текстов (документов) в коллекции. \n",
    "- $|\\{d_i \\in D \\mid t \\in d_i\\}|$ — сколько текстов содержат слово $t$.\n",
    "\n",
    "Логарифмирование здесь проводится с целью уменьшить масштаб весов, ибо зачастую в корпусах присутствует очень много текстов.\n",
    "\n",
    "IDF показывает, $\\underline{насколько}$ $\\underline{редкое}$ это слово среди всех текстов.  \n",
    "- Если слово встречается почти во всех текстах (например, \"и\"), **IDF будет маленьким**. \n",
    "- Если слово встречается только в паре статей — **IDF будет большим**.  \n",
    "\n",
    "**Пример:**  Предположим, у нас 1000 статей ($|D|$ = 1000),  и слово \"нейросеть\" есть только в 10 из них: - IDF(\"нейросеть\", D) = log(1000 / 10) = log(100) = 2 (если берём десятичный логарифм)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Общая формула\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "$$\n",
    "\n",
    "- Если слово часто встречается в **конкретном тексте** (большой TF) и при этом редко встречается в **других текстах** (большой IDF), то **его вес будет большим**. \n",
    "- Если слово встречается везде — его вес будет маленьким. \n",
    "  \n",
    "**Пример:**  \n",
    "- \"машина\" встречается часто в тексте и редко вообще — TF-IDF будет высоким и слово считается важным для этого текста. \n",
    "- \"и\" встречается часто в тексте, но и вообще везде — TF-IDF маленький, оно считается неважным.\n",
    "\n",
    "\n",
    "**Есть и другие подходы к определению [TF и IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3dff947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(texts_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5a586c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['like', 'my', 'cat', 'is', 'the', 'most', 'perfect', 'this', 'or', 'bread'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5c55a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40eb0d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.42544054, 0.        , 0.72033345, 0.        ,\n",
       "        0.54783215, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.50130994, 0.32276391, 0.        , 0.42439575,\n",
       "        0.32276391, 0.        , 0.42439575, 0.42439575, 0.        ],\n",
       "       [0.33976626, 0.20067143, 0.516802  , 0.        , 0.        ,\n",
       "        0.        , 0.33976626, 0.        , 0.        , 0.67953252]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ff664a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bread</th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>most</th>\n",
       "      <th>my</th>\n",
       "      <th>or</th>\n",
       "      <th>perfect</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501310</td>\n",
       "      <td>0.322764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.322764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.339766</td>\n",
       "      <td>0.200671</td>\n",
       "      <td>0.516802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.679533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bread       cat        is      like      most        my        or  \\\n",
       "0  0.000000  0.425441  0.000000  0.720333  0.000000  0.547832  0.000000   \n",
       "1  0.000000  0.501310  0.322764  0.000000  0.424396  0.322764  0.000000   \n",
       "2  0.339766  0.200671  0.516802  0.000000  0.000000  0.000000  0.339766   \n",
       "\n",
       "    perfect       the      this  \n",
       "0  0.000000  0.000000  0.000000  \n",
       "1  0.424396  0.424396  0.000000  \n",
       "2  0.000000  0.000000  0.679533  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), columns=tfidf_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aec7d647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bread</th>\n",
       "      <th>cat</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>most</th>\n",
       "      <th>my</th>\n",
       "      <th>or</th>\n",
       "      <th>perfect</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345205</td>\n",
       "      <td>0.584483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.322764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.339766</td>\n",
       "      <td>0.200671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.679533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bread       cat         i        is      like      most        my  \\\n",
       "0  0.000000  0.345205  0.584483  0.000000  0.584483  0.000000  0.444514   \n",
       "1  0.000000  0.501310  0.000000  0.322764  0.000000  0.424396  0.322764   \n",
       "2  0.339766  0.200671  0.000000  0.516802  0.000000  0.000000  0.000000   \n",
       "\n",
       "         or   perfect       the      this  \n",
       "0  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.424396  0.424396  0.000000  \n",
       "2  0.339766  0.000000  0.000000  0.679533  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec_2 = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X_2 = tfidf_vec_2.fit_transform(texts_tokenized)\n",
    "\n",
    "pd.DataFrame(X_2.toarray(), columns=tfidf_vec_2.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b6a83",
   "metadata": {},
   "source": [
    "## 5. Решение задачи с текстовыми данными\n",
    "\n",
    "Будем решать задачу классификации твитов по тональности. Возьмём датасет из твитов, в котором про каждый твит известно, как он эмоционально окрашен: положительно или отрицательно. Задача: предсказывать эмоциональную окраску. Классификацию по тональности используют, например, в рекомендательных системах, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
    "\n",
    "Скачиваем куски датасета ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f275105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-25 23:02:49--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
      "Распознаётся www.dropbox.com (www.dropbox.com)… 2620:100:6022:18::a27d:4212, 162.125.70.18\n",
      "Подключение к www.dropbox.com (www.dropbox.com)|2620:100:6022:18::a27d:4212|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 302 Found\n",
      "Адрес: https://www.dropbox.com/scl/fi/6mg7rw3wltux83q2o4ah4/positive.csv?rlkey=cvruhzofza9kkfxwzyp2vskfd [переход]\n",
      "--2025-08-25 23:02:50--  https://www.dropbox.com/scl/fi/6mg7rw3wltux83q2o4ah4/positive.csv?rlkey=cvruhzofza9kkfxwzyp2vskfd\n",
      "Повторное использование соединения с [www.dropbox.com]:443.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 302 Found\n",
      "Адрес: https://ucd413931a677aa771e6d2f1f607.dl.dropboxusercontent.com/cd/0/inline/CwGw-WgWVF8haRjTbwz3HMpTGXXTWtwpVziWY5skUFgKmmAaHkQ_Z44orMW1OfwZ4w-mFBLcJPsxUUPy7B1xg0HIQziJJ7nrzIUr10f26TFvm_UDjFC4UO0F0YV_7oJXawc/file# [переход]\n",
      "--2025-08-25 23:02:50--  https://ucd413931a677aa771e6d2f1f607.dl.dropboxusercontent.com/cd/0/inline/CwGw-WgWVF8haRjTbwz3HMpTGXXTWtwpVziWY5skUFgKmmAaHkQ_Z44orMW1OfwZ4w-mFBLcJPsxUUPy7B1xg0HIQziJJ7nrzIUr10f26TFvm_UDjFC4UO0F0YV_7oJXawc/file\n",
      "Распознаётся ucd413931a677aa771e6d2f1f607.dl.dropboxusercontent.com (ucd413931a677aa771e6d2f1f607.dl.dropboxusercontent.com)… 2620:100:6022:15::a27d:420f, 162.125.70.15\n",
      "Подключение к ucd413931a677aa771e6d2f1f607.dl.dropboxusercontent.com (ucd413931a677aa771e6d2f1f607.dl.dropboxusercontent.com)|2620:100:6022:15::a27d:420f|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 26233379 (25M) [text/plain]\n",
      "Сохранение в: «positive.csv»\n",
      "\n",
      "positive.csv        100%[===================>]  25,02M  19,5MB/s    за 1,3s    \n",
      "\n",
      "2025-08-25 23:02:52 (19,5 MB/s) - «positive.csv» сохранён [26233379/26233379]\n",
      "\n",
      "--2025-08-25 23:02:53--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
      "Распознаётся www.dropbox.com (www.dropbox.com)… 2620:100:6022:18::a27d:4212, 162.125.70.18\n",
      "Подключение к www.dropbox.com (www.dropbox.com)|2620:100:6022:18::a27d:4212|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 302 Found\n",
      "Адрес: https://www.dropbox.com/scl/fi/wui0xz78kpna56690uej4/negative.csv?rlkey=309xeou9u3rtbejw9stb13wfr [переход]\n",
      "--2025-08-25 23:02:53--  https://www.dropbox.com/scl/fi/wui0xz78kpna56690uej4/negative.csv?rlkey=309xeou9u3rtbejw9stb13wfr\n",
      "Повторное использование соединения с [www.dropbox.com]:443.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 302 Found\n",
      "Адрес: https://ucc04b1aaba13799f9effe73a9ea.dl.dropboxusercontent.com/cd/0/inline/CwHepUtQ0q002f1hkGn_tRs2JEBkaUWoVzrHuhhQuuQXZTD5XZHF5FzFfBUr8dglp_EBz6tkPCy827YyapTCXc_CtwAlucX6fbXJe5Vg3Nt1ofaF0KmCMuMB35ztessB8ig/file# [переход]\n",
      "--2025-08-25 23:02:54--  https://ucc04b1aaba13799f9effe73a9ea.dl.dropboxusercontent.com/cd/0/inline/CwHepUtQ0q002f1hkGn_tRs2JEBkaUWoVzrHuhhQuuQXZTD5XZHF5FzFfBUr8dglp_EBz6tkPCy827YyapTCXc_CtwAlucX6fbXJe5Vg3Nt1ofaF0KmCMuMB35ztessB8ig/file\n",
      "Распознаётся ucc04b1aaba13799f9effe73a9ea.dl.dropboxusercontent.com (ucc04b1aaba13799f9effe73a9ea.dl.dropboxusercontent.com)… 2620:100:6022:15::a27d:420f, 162.125.70.15\n",
      "Подключение к ucc04b1aaba13799f9effe73a9ea.dl.dropboxusercontent.com (ucc04b1aaba13799f9effe73a9ea.dl.dropboxusercontent.com)|2620:100:6022:15::a27d:420f|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 24450101 (23M) [text/plain]\n",
      "Сохранение в: «negative.csv»\n",
      "\n",
      "negative.csv        100%[===================>]  23,32M  18,4MB/s    за 1,3s    \n",
      "\n",
      "2025-08-25 23:02:56 (18,4 MB/s) - «negative.csv» сохранён [24450101/24450101]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
    "# !wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0bd2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3c12e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# считываем данные и заполняем общий датасет\n",
    "positive = pd.read_csv(\"positive.csv\", sep=\";\", usecols=[3], names=[\"text\"])\n",
    "positive[\"label\"] = \"positive\"\n",
    "negative = pd.read_csv(\"negative.csv\", sep=\";\", usecols=[3], names=[\"text\"])\n",
    "negative[\"label\"] = \"negative\"\n",
    "df = pd.concat([positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c3ef290e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  @first_timee хоть я и школота, но поверь, у на...  positive\n",
       "1  Да, все-таки он немного похож на него. Но мой ...  positive\n",
       "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...  positive\n",
       "3  RT @digger2912: \"Кто то в углу сидит и погибае...  positive\n",
       "4  @irina_dyshkant Вот что значит страшилка :D\\nН...  positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111918</th>\n",
       "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111919</th>\n",
       "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111920</th>\n",
       "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111921</th>\n",
       "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111922</th>\n",
       "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label\n",
       "111918  Но не каждый хочет что то исправлять:( http://...  negative\n",
       "111919  скучаю так :-( только @taaannyaaa вправляет мо...  negative\n",
       "111920          Вот и в школу, в говно это идти уже надо(  negative\n",
       "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  negative\n",
       "111922  Такси везет меня на работу. Раздумываю приплат...  negative"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head(), df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "62edfb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226834, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b143cc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 226834 entries, 0 to 111922\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    226834 non-null  object\n",
      " 1   label   226834 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a82e9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разобьем исходный датасет на train и test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f8b67",
   "metadata": {},
   "source": [
    "#### n-граммы\n",
    "\n",
    "n-граммы — это последовательности n токенов из исходного текста. В простейшем случае это могут быть последовательности из букв или последовательности из слов. Давайте посмотрим подробнее на примере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e31ea454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ba27eb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Если б мне платили каждый раз\".split()\n",
    "list(ngrams(sent, 1))  # униграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ba0944b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б'),\n",
       " ('б', 'мне'),\n",
       " ('мне', 'платили'),\n",
       " ('платили', 'каждый'),\n",
       " ('каждый', 'раз')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 2))  # биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1eacd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне'),\n",
       " ('б', 'мне', 'платили'),\n",
       " ('мне', 'платили', 'каждый'),\n",
       " ('платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 3))  # триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5221a9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
       " ('б', 'мне', 'платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 5))  # ... пентаграммы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a2fca",
   "metadata": {},
   "source": [
    "В качестве альтернативы можно пользоваться `CountVectorizer`, который работает так:\n",
    "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности количества токенов в нашем словаре\n",
    "* заполняет каждый i-тый элемент количеством вхождений токена в данный документ\n",
    "\n",
    "Параметр `ngram_range` отвечает за то, какие n-граммы мы используем в качестве фичей:\n",
    "- `ngram_range=(1, 1)` — униграммы\n",
    "- `ngram_range=(3, 3)` — триграммы\n",
    "- `ngram_range=(1, 3)` — униграммы, биграммы и триграммы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5fe27",
   "metadata": {},
   "source": [
    "### 5.1 Обучение моделей\n",
    "\n",
    "Обучим наш первый бейзлайн — логрег на униграммах!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.77      0.76     27957\n",
      "    positive       0.77      0.76      0.77     28752\n",
      "\n",
      "    accuracy                           0.77     56709\n",
      "   macro avg       0.76      0.77      0.76     56709\n",
      "weighted avg       0.77      0.77      0.77     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(x_train)  # bow — bag of words (мешок слов)\n",
    "bow_test = vec.transform(x_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=500, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631ed08",
   "metadata": {},
   "source": [
    "Попробуем сделать то же самое для биграмм и триграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0bc0ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.74      0.72     27957\n",
      "    positive       0.73      0.69      0.71     28752\n",
      "\n",
      "    accuracy                           0.71     56709\n",
      "   macro avg       0.71      0.71      0.71     56709\n",
      "weighted avg       0.71      0.71      0.71     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec2 = CountVectorizer(ngram_range=(2, 2))\n",
    "bow2 = vec2.fit_transform(x_train)  # bow — bag of words (мешок слов)\n",
    "bow_test2 = vec2.transform(x_test)\n",
    "\n",
    "scaler2 = MaxAbsScaler()\n",
    "bow2 = scaler2.fit_transform(bow2)\n",
    "bow_test2 = scaler2.transform(bow_test2)\n",
    "\n",
    "clf2 = LogisticRegression(max_iter=500, random_state=42)\n",
    "clf2.fit(bow2, y_train)\n",
    "pred2 = clf2.predict(bow_test2)\n",
    "print(classification_report(y_test, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "20eb8b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.47      0.57     27957\n",
      "    positive       0.61      0.82      0.70     28752\n",
      "\n",
      "    accuracy                           0.65     56709\n",
      "   macro avg       0.67      0.65      0.64     56709\n",
      "weighted avg       0.67      0.65      0.64     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec3 = CountVectorizer(ngram_range=(3, 3))\n",
    "bow3 = vec3.fit_transform(x_train)  # bow — bag of words (мешок слов)\n",
    "bow_test3 = vec3.transform(x_test)\n",
    "\n",
    "scaler3 = MaxAbsScaler()\n",
    "bow3 = scaler3.fit_transform(bow3)\n",
    "bow_test3 = scaler3.transform(bow_test3)\n",
    "\n",
    "clf3 = LogisticRegression(max_iter=500, random_state=42)\n",
    "clf3.fit(bow3, y_train)\n",
    "pred3 = clf3.predict(bow_test3)\n",
    "print(classification_report(y_test, pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a20585",
   "metadata": {},
   "source": [
    "Как видно биграммы и триграммы дают худшую точность относительно униграмм."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8280d36",
   "metadata": {},
   "source": [
    "А теперь повторим процедуру для TF-IDF (на униграммах)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1dd43b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.75      0.76     27957\n",
      "    positive       0.76      0.78      0.77     28752\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
    "vec_train = vec.fit_transform(x_train)\n",
    "vec_test = vec.transform(x_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "vec_train = scaler.fit_transform(vec_train)\n",
    "vec_test = scaler.transform(vec_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=500, random_state=42)\n",
    "clf.fit(vec_train, y_train)\n",
    "pred_tfidf = clf.predict(vec_test)\n",
    "print(classification_report(y_test, pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9bcf70",
   "metadata": {},
   "source": [
    "### 5.2 О важности эксплоративного анализа\n",
    "\n",
    "Иногда пунктуация бывает и не шумом. Главное — отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b7bbb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.97      0.96     27957\n",
      "    positive       0.97      0.95      0.96     28752\n",
      "\n",
      "    accuracy                           0.96     56709\n",
      "   macro avg       0.96      0.96      0.96     56709\n",
      "weighted avg       0.96      0.96      0.96     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize)\n",
    "bow = vec.fit_transform(x_train)\n",
    "bow_test = vec.transform(x_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=500, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4470197",
   "metadata": {},
   "source": [
    "Как мы видим отказ от очистки от пунктуации (в базовом случае, используется внутренний токенизатор, который как мы помним работает таким паттерном `token_pattern=r'(?u)\\b\\w\\w+\\b'`, при указании параметра `tokenizer=word_tokenize` мы ссылаемся на внешний токенизатор и внутренний токенизатор уже не вмешивается в работу), привел к внезапному росту всех метрик, значит в данном случаи какие-то пунктуационные символы являются очень значимыми. Посмотрим на наиболее значимый из них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ea94247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "')'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names_out()[np.argmax(clf.coef_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0d6dc",
   "metadata": {},
   "source": [
    "Можно построить предскозание основываясь на наличии только одного этого токена и посмотреть на точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bd934303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92     27957\n",
      "    positive       1.00      0.83      0.91     28752\n",
      "\n",
      "    accuracy                           0.91     56709\n",
      "   macro avg       0.93      0.92      0.91     56709\n",
      "weighted avg       0.93      0.91      0.91     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cool_token = vec.get_feature_names_out()[np.argmax(clf.coef_)]\n",
    "pred = [\"positive\" if cool_token in tweet else \"negative\" for tweet in x_test]\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "feca3eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“@Fashionbar_uz: Ladies Monday . Каждый понедельник для всех девушек кальяны от заведения. #fashionbar http://t.co/ZlvxOcZPR4” zaviduyu))\n",
      "Гримерка-лук))) #evabristol #performance #gig #vocaldiva #гастроли #гитис #театр http://t.co/S60OvyyTS6\n",
      "RT @alivfedorov: http://t.co/DvYLJaPHxR Девушки это самые хитрые создания! так что даже не пытайся их обмануть!:)\n",
      "@pavelsheremet @ukrpravda_news @varlamov хотя исторически правильно желто-синий:)\n",
      "Понятия не имею чем меня привлекла эта картинка!)))http://t.co/twqP8zyh1A\n",
      "@Sveta12126 ну или пусть Лиама пришлет ко мне) своего младшенького. Как сказала Кэтрин: \"Это нормально - любить двоих\" ахах\n",
      "Вышел в свет новый каталог запасных частей и деталей ТМК! Звоните - подарим :)\n",
      "@u_alekseeva_17 видужуй:3\n",
      "Я там тепер буду кожну середу)\n",
      "29-й выпуск Дроидкаста будет не против ваших плюсов на Хабре   ;)\n",
      "@nemoniga а я вот знаю :) нам на политической географии рассказывал душечка Гурин\n"
     ]
    }
   ],
   "source": [
    "cool_token = vec.get_feature_names_out()[np.argmax(clf.coef_)]\n",
    "tweets_with_cool_token = [tweet for tweet in x_train if cool_token in tweet]\n",
    "np.random.seed(42)\n",
    "for tweet in np.random.choice(tweets_with_cool_token, size=10, replace=False):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2279194",
   "metadata": {},
   "source": [
    "Как мы увидели модель классификации, которая делает предсказание только исходя из факта наличичя символа `)` в тексте сравнима по точности с нашей самой точной моделью линейной регрессии. \n",
    "\n",
    "**Вывод:** Для данного набора данных, наличиче в тексте символа `)` - является значимым фактором, чтобы сделать предсказание в пользу класса `positive`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d7ed6",
   "metadata": {},
   "source": [
    "### 5.3 Символьные n-граммы\n",
    "\n",
    "Теперь в качестве признаков используем, например, униграммы символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d6262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.97      0.98     27957\n",
      "    positive       0.98      0.99      0.98     28752\n",
      "\n",
      "    accuracy                           0.98     56709\n",
      "   macro avg       0.98      0.98      0.98     56709\n",
      "weighted avg       0.98      0.98      0.98     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), analyzer=\"char\") #оставляем только едиинчные символы в качестве токенов\n",
    "bow = vec.fit_transform(x_train)\n",
    "bow_test = vec.transform(x_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=500, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e03284",
   "metadata": {},
   "source": [
    "В общем-то, теперь уже понятно, почему на этих данных здесь 1. Так или иначе, на символах классифицировать тоже можно: для некоторых задач (например, для определения языка) признаки-символьные n-граммы могут внести серьезный вклад в качество модели. Ещё одна замечательная особенность признаков-символов: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готовых анализаторов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98214b49",
   "metadata": {},
   "source": [
    "## 6. Регулярные выражения\n",
    "\n",
    "Часто бывает так, что для конкретного случая нужен особый способ токенизации, и надо самостоятельно написать правило шаблонного типа для определения того, что такое токен. Или, например, перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее. В таких задачах могут помочь регулярные выражения. Навык полезный, давайте в нём тоже потренируемся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "644613b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b690325",
   "metadata": {},
   "source": [
    "### findall\n",
    "возвращает список всех найденных совпадений\n",
    "\n",
    "- `?` : ноль или одно повторение\n",
    "- `*` : ноль или более повторений\n",
    "- `+` : одно или более повторений\n",
    "- `.`: любой символ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3d426ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcd', 'abca']\n"
     ]
    }
   ],
   "source": [
    "result = re.findall(\"ab+c.\", \"abcdefghijkabcabcxabc\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e01b0f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abbbca']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"ab+c.\", \"abbbca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437f08c",
   "metadata": {},
   "source": [
    "### split\n",
    "разделяет строку по заданному шаблону\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "291d29bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itsy', ' bitsy', ' teenie', ' weenie']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(\",\", \"itsy, bitsy, teenie, weenie\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a90d9b",
   "metadata": {},
   "source": [
    "можно указать максимальное количество разбиений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "63ce9d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itsy', ' bitsy', ' teenie, weenie']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(\",\", \"itsy, bitsy, teenie, weenie\", maxsplit=2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a1765",
   "metadata": {},
   "source": [
    "### sub\n",
    "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
    "\n",
    "параметры: (pattern, repl, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "278e0f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbcbbc\n"
     ]
    }
   ],
   "source": [
    "result = re.sub(\"a\", \"b\", \"abcabc\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b74981",
   "metadata": {},
   "source": [
    "При этом в качестве repl, можно передавать не только строку, но и функцию, которая принимает на вход [Match](https://docs.python.org/3/library/re.html#match-objects) объект. Можно делать что-то типо этого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "443ccbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(a#1)bc(a#2)bc'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "\n",
    "def count(match):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    return f\"(a#{counter})\"\n",
    "\n",
    "\n",
    "re.sub(\"a\", count, \"abcabc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999ae04",
   "metadata": {},
   "source": [
    "Кстати, c объектами типа re.Match работают и многие другие методы re. Например, метод re.finditer в отличии от re.findall будет возвращать те самые re.Match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "35c905a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='abcd'>\n",
      "<re.Match object; span=(11, 15), match='abca'>\n"
     ]
    }
   ],
   "source": [
    "for match in re.finditer(\"ab+c.\", \"abcdefghijkabcabcxabc\"):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30e6a3",
   "metadata": {},
   "source": [
    "Помимо найденных строчек объекты Match также, например, содержат информацию о позиции найденного \"совпадения\" в строке (span)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c23d23",
   "metadata": {},
   "source": [
    "### compile\n",
    "компилирует регулярное выражение в отдельный объект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d1481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример: построение списка всех слов строки:\n",
    "prog = re.compile(\"[А-Яа-яё\\-]+\")\n",
    "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7739db6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@gmail.com', '@test.in', '@analyticsvidhya.com', '@rest.biz']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Так можно вернуть списко всех доменов из строки с разынми почтами\n",
    "emails = \"abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\"\n",
    "domains = re.compile(r'@[\\w.-]+')\n",
    "domains.findall(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1bbedd",
   "metadata": {},
   "source": [
    "## 7. Немножко про категориальные признаки\n",
    "\n",
    "Мы уже говорили, что кодировать категориальные признаки просто в виде чисел — не очень хорошая идея. Это задаёт некоторый порядок, которого на категориальных переменных может и не быть. Существует три основных способа обработки категориальных значений:\n",
    "- Label encoding\n",
    "- One-hot-кодирование\n",
    "- Счётчики (CTR, mean-target кодирование) — каждый категориальный признак заменяется на среднее значение целевой переменной по всем объектам, имеющим одинаковое значение в этом признаке\n",
    "\n",
    "Основная идея счетчиков заключается в том, что нам важны не сами категории, а значения целевой переменной, которые имеют объекты этой категории. Каждый категориальный признак мы заменим средним значением целевой переменной по всем объектам этой же категории. Формально это можно записать так:\n",
    "$$\n",
    "g_j(x, X) = \\frac{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)][y_i = +1]}{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)]}\n",
    "$$\n",
    "\n",
    "Сравним два последних метода на нашем любимом Титанике, но на этот раз не будем сильно заморачиваться с обработкой пропусков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c9629b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a9959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\n",
    "#     \"https://raw.githubusercontent.com/iad34/seminars/master/materials/data_sem1.csv\",\n",
    "#     sep=\";\",\n",
    "# )\n",
    "\n",
    "# data[\"Age\"] = data[\"Age\"].fillna(data.groupby(\"Pclass\")[\"Age\"].transform(\"median\"))\n",
    "# data.drop(\"Cabin\", axis=1, inplace=True)\n",
    "# data.drop(\"Name\", axis=1, inplace=True)\n",
    "# data.drop(\"Ticket\", axis=1, inplace=True)\n",
    "# data.dropna(inplace=True)\n",
    "# data.to_csv('cat_feat_enc_exanple.csv')\n",
    "\n",
    "data = pd.read_csv('cat_feat_enc_exanple.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66daa7a2",
   "metadata": {},
   "source": [
    "### 7.1 OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b7fda5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Sex_unknown</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass   Age  SibSp  Parch     Fare  Sex_male  Sex_unknown  \\\n",
       "0            1       3  22.0      1      0   7.2500      True        False   \n",
       "1            2       1  38.0      1      0  71.2833     False        False   \n",
       "2            3       3  26.0      0      0   7.9250     False        False   \n",
       "3            4       1  35.0      1      0  53.1000     False        False   \n",
       "4            5       3  35.0      0      0   8.0500      True        False   \n",
       "\n",
       "   Embarked_Q  Embarked_S  \n",
       "0       False        True  \n",
       "1       False       False  \n",
       "2       False        True  \n",
       "3       False        True  \n",
       "4       False        True  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ohe = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "y = data_ohe[\"Survived\"]\n",
    "data_ohe = data_ohe.drop([\"Survived\"], axis=1)\n",
    "data_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "87e2805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.77\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_ohe, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=300)\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(f\"ROC AUC: {roc_auc_score(prediction, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1567de",
   "metadata": {},
   "source": [
    "### 7.2 Mean target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fdaddcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "864e1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_target = X_train.groupby(\"Sex\")[\"Survived\"].mean()\n",
    "X_train.loc[:, \"Sex\"] = X_train[\"Sex\"].replace(mean_target)\n",
    "X_test.loc[:, \"Sex\"] = X_test[\"Sex\"].replace(mean_target)\n",
    "\n",
    "mean_target_e = X_train.groupby(\"Embarked\")[\"Survived\"].mean()\n",
    "X_train.loc[:, \"Embarked\"] = X_train[\"Embarked\"].replace(mean_target_e)\n",
    "X_test.loc[:, \"Embarked\"] = X_test[\"Embarked\"].replace(mean_target_e)\n",
    "\n",
    "X_train.drop([\"Survived\"], axis=1, inplace=True)\n",
    "X_test.drop([\"Survived\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0eb6b8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.79\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=300)\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(f\"ROC AUC: {roc_auc_score(prediction, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a0aa8b",
   "metadata": {},
   "source": [
    "Кодирование признаков с помощью счетчиков приведённом выше может приводить к переобучению.\n",
    "\n",
    "Чтобы бороться с этим, можно экспериментировать с разными модификациями:\n",
    "\n",
    "1. Вычислять значение счётчика по всем объектам расположенным выше в датасете (например, если у нас выборка отсортирована по времени)\n",
    "2. Вычислять по фолдам, то есть делить выборку на некоторое количество частей и подсчитывать значение признаков по всем фолдам кроме текущего (как делается в кросс-валидации)\n",
    "3. Добавлять шум в посчитанные признаки"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
